---
title: "MMSE score prediction"
author: "Luiz Felipe Martucci"
date: "2023-01-31"
output:  
  html_document:
       keep_md: true
  pdf_document: default
---

```{r setup, message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libs, message=FALSE, warning=FALSE, include=FALSE}
(\(x){
  sapply(x, function(x) if(!x %in% installed.packages()){
    install.packages(x, dependencies = T)
  })
  sapply(x, library, character.only=T)
})(c("tidyverse", "gt", "caret", "doParallel", "glmnet", "pROC", "kernlab", "naivebayes"))


```

```{r fns, message=FALSE, warning=FALSE, include=FALSE}


scale_min_max <- function(var){
  (var - min(var)) / (max(var) - min(var))
}
  

filter_vec <- function(exp, .data = NULL){
  index <- exp
  vec <- as.character(substitute(exp))[2]
  vec <- substitute(vec)
  vec <- eval(parse(text = vec))
  if(is.null(.data)){
  vec[index]
  }else{
    .data[index]
  }
}


Bar_plot_theme <- function(){
  
  font <- "Arial"
  
  
  theme(
    #plot.margin = margin(16, 16, 16, 16),
    #legend.position = "top",
    plot.title = ggplot2::element_text(family = font, 
                                       size = 16, 
                                       face = "bold",
                                       color = "#222222"),
    plot.subtitle = ggplot2::element_text(family = font, 
                                          size = 22,
                                          margin = ggplot2::margin(9, 0, 9, 0)), 
    legend.text.align = 0,
    legend.background = ggplot2::element_blank(),
    legend.title = ggplot2::element_blank(),
    legend.key = ggplot2::element_blank(),
    axis.title.x = ggplot2::element_blank(),
    axis.title.y = ggplot2::element_text(
      color = "#222222",
      size = 14,
      margin = margin(5, r = 10)
    ),
    axis.text = ggplot2::element_text(
      family = font,
      size = 14,
      color = "#222222"
    ),
    axis.text.x        = ggplot2::element_text(margin = ggplot2::margin(5, b = 10)),
    axis.ticks         = ggplot2::element_line(color = "#222222"),
    axis.line          = ggplot2::element_line(color = "#222222"),
    panel.grid.minor   = ggplot2::element_blank(),
    panel.grid.major.y = ggplot2::element_blank(),
    panel.grid.major.x = ggplot2::element_blank(),
    panel.background   = ggplot2::element_blank(),
    plot.background    = ggplot2::element_blank(),
    strip.background   = ggplot2::element_rect(fill = "white"),
    #strip.text = ggplot2::element_text(size = 22, hjust = 0),
    aspect.ratio       = 1.21
  )
}

```


## Introduction

This project aims to predict if an elderly has fallen in the past six months by using information about his gait pattern.

The data used in this project is from the work of Noh, B., Youm, C., Goh, E., et al. (2021), published in Nature Scientific Reports. The dataset contains 746 observations, and the target variable "History of Falls" is highly unbalanced. Therefore we will need to optimize our models to deal with this challenge.

```{r data, message=FALSE, warning=FALSE, include=FALSE}

df_full <- (\(){
  d1 <- readxl::read_excel("data/41598_2021_91797_MOESM1_ESM.xlsx", 
    skip = 1) %>% 
  mutate(Number = as.numeric(Number)) %>% 
  filter(!is.na(Number))

d2 <-  readxl::read_excel("data/41598_2021_91797_MOESM1_ESM.xlsx", 
    sheet = "Environmental characteristics", 
    skip = 1) 

  
data <- left_join(d1, d2) %>% 
  select(-c(`education year`, Fear_of_fall, `Fall risk`)) 


#Correcting variable type
d_adj <- data %>% mutate(across(where(is.numeric), ~ if(max(.) <= 5) factor(.)))

# Some variables where incorrectly selected
index <- grepl("time|speed|length", colnames(d_adj))
correct_vars <- colnames(d_adj)[!index]


d_adj <- d_adj %>%
  select(all_of(correct_vars)) %>% 
  cbind(Number = data$Number)


data <- data %>% select(-all_of(correct_vars)) %>% 
  left_join(d_adj, by="Number") %>% 
  mutate(across(where(is.factor), ~ paste0("lv", .)),
         across(where(is.character),as.factor))

# correcting names
namessub <- gsub("%", "pct", colnames(data))
namessub <- gsub("[\\(|\\) ]", "", namessub)


names(data) <- namessub

return(data)

  
})()


```

```{r echo=FALSE}
plots <- list()
#scales::breaks_pretty()
plots$p1 <- df_full %>% 
  mutate(History_of_fall = ifelse(History_of_fall == "lv0", "No", "Yes")) %>% 
  ggplot(aes(History_of_fall, fill= History_of_fall)) +
  geom_bar(show.legend = FALSE)+
  ylab("History of falls") +
  scale_fill_manual(values = c("#00589B", "#C43362")) +
  scale_y_continuous(expand = c(0, NA),
                     limits = c(NA, NA))+
  Bar_plot_theme()

plots$p1

ggsave("p1.png",
       plots$p1,
       device = "png",
       units = "cm",
       width = 8,
       dpi= "retina",
       bg = "white"
)

```

## Preparing our data
Before training any model, we need to check for outliers and skews in data. Otherwise, those outliers can influence our model and scaling parameters. Even so, in this first version, we will train our first model on raw data to highlight how each step improves our model. As a first model, we will use logistic regression with lasso, ridge, and elastic net regularization.

To train and later evaluate our model, we will split the data into two sets: training and testing. The training set will contain 80% of the data, and the remaining 20% will be used as a testing set

```{r split data}
(\(data){
  set.seed(44)
  index_trainData <- caret::createDataPartition(data$History_of_fall,
                                                times=1,
                                                p=.8,
                                                list=FALSE)

  train_data <<- data[index_trainData,] 
  test_data <<- data[-index_trainData,]
})(df_full)

```

Next, with the data split, we will scale our values using the min-max approach. We will store the min-max values of training data and use them to normalize the testing data, avoiding data leakage at this step.

```{r scaling}
scale_parameters <- (\(df){

  
  
 vars_max <- df %>% summarise(across(where(is.numeric), max))
 vars_min <- df %>% summarise(across(where(is.numeric), min))


 scale_params <- rbind(vars_max, vars_min) %>% as.data.frame()

 rownames(scale_params) <- c("max", "min")

 return(scale_params)
  
})(train_data)


train_scl <- train_data %>% 
  mutate(across(where(is.numeric), scale_min_max))


test_scl <- map2(test_data %>% select(where(is.numeric)),
                 scale_parameters, 
                 function(x, y) (x - y[2]) / (y[1] - y[2])
                 ) %>%
  as.data.frame() %>% 
  cbind(test_data %>% select(- where(is.numeric)))
  



```

## Machine learning algorithms

Now, we are ready to train our first model! We will do it using 5-fold cross-validation and searching for the best alpha and lambda parameters at the 0-1 range. 

```{r glm, message=FALSE, warning=FALSE}

glm_model <- (\(data){

  
  train_control <- trainControl(method          = "repeatedcv", 
                                number          = 5,
                                repeats         = 3,
                                savePredictions = "final",
                                classProbs      = TRUE,
                                summaryFunction = twoClassSummary,
                                allowParallel   = FALSE,
                                sampling        = "smote")

  set.seed(44)  
  model <- caret::train(History_of_fall ~ .,
               data      = data,
               trControl = train_control,
               method    = "glmnet",
               family    = "binomial",
               metric    = "ROC",
               tuneGrid  = expand.grid(
                  .lambda= seq(0, 1, length.out = 6),
                  .alpha = seq(0, 1, length.out = 5))) # to get area under the ROC curve

  

  
  perf_eval <-  model$results$Youden %>% max()
  print(perf_eval)
  
  system("say terminei")
  
  return(model)
         
})(train_scl %>% select(-No_of_fall))


glm_model$results$ROC %>% max()
pROC::auc(train_scl$History_of_fall, predict(glm_model, train_scl, type= "prob")[,2])
pROC::auc(test_scl$History_of_fall, predict(glm_model, test_scl, type= "prob")[,2])






```

Our basic trained model averaged an auROC of **0.59** on cross-validation. While the auROC achieved on training data was **0.66** and on testing data **0.55**.




## References 
Noh, B., Youm, C., Goh, E., et al. XGBoost-based machine learning approach to predict the risk of fall in older adults using gait outcomes. Sci Rep 11, 12183 (2021). https://doi.org/10.1038/s41598-021-91797-w

